## Hi there! 👋

I'm **QI QIN(秦萁)**, a Researcher at Shanghai AI Lab and incoming PhD student at the University of Sydney.

## 🔬 Research Interests

### 🎨 **Generative Models**
- **Image Generation**:
  - [Lumina-Image 2.0](https://github.com/Alpha-VLLM/Lumina-Image-2.0) (![GitHub stars](https://img.shields.io/github/stars/Alpha-VLLM/Lumina-Image-2.0?style=social)) - A Unified and Efficient Image Generative Framework.
  - [Lumina-mGPT](https://github.com/Alpha-VLLM/Lumina-mGPT) (![GitHub stars](https://img.shields.io/github/stars/Alpha-VLLM/Lumina-mGPT?style=social)) - Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining.
- **Video Generation**:
  - [Lumina-Video](https://github.com/Alpha-VLLM/Lumina-Video) (![GitHub stars](https://img.shields.io/github/stars/Alpha-VLLM/Lumina-Video?style=social)) - Efficient and Flexible Video Generation with Multi-scale Next-DiT.

### 🧠 **Unified Image Generation and Understanding Models** 
- **AutoRegressive**:
  - [Lumina-mGPT-2.0](https://github.com/Alpha-VLLM/Lumina-mGPT-2.0) (![GitHub stars](https://img.shields.io/github/stars/Alpha-VLLM/Lumina-mGPT-2.0?style=social)) - Stand-Alone AutoRegressive Image Modeling.
- **Diffusion Large Language Model**:
  - [Lumina-DiMOO](https://github.com/Alpha-VLLM/Lumina-DiMOO) (![GitHub stars](https://img.shields.io/github/stars/Alpha-VLLM/Lumina-DiMOO?style=social)) - An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding.

### 🏷️ **Multimodal Captioner**
- [OmniCaptioner](https://github.com/Alpha-Innovator/OmniCaptioner/tree/main) (![GitHub stars](https://img.shields.io/github/stars/Alpha-Innovator/OmniCaptioner?style=social)) - One Captioner to Rule Them All.

📧 **Contact**: Feel free to drop me an email (qinqi.9812@gmail.com) if you're interested.
